---
title: "Final Report"
subtitle: "<h2><u>Data Science and Predictive Analytics (HS650)</u></h2>"
author: "<h3>Tarrik Quneibi</h3>"
date: "`r format(Sys.time(), '%B %Y')`"
output:
  html_document:
    theme: journal
    highlight: tango
    toc: true
    number_sections: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_folding: show
    self_contained: yes
---

 * Fall 2022, DSPA (HS650)
 * SID: 1236 
 * UMich E-mail: tarrikq@umich.edu
 * I certify that the following paper represents my own independent work and conforms with the guidelines of academic honesty described in the UMich student handbook.
 * Remember that students are allowed and encouraged to discuss, on a conceptual level, the problems with your class mates, however, this can not involve the exchange of actual code, printouts, solutions, e-mails or other explicit electronic or paper handouts.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message=FALSE, warning=FALSE, echo=FALSE}
library(readr)
library(rvest)
library(plotly)
library(ggplot2)
library(lubridate)
library(Amelia)
library(tidyverse)
library(Hmisc)
library(neuralnet)
library(keras)
library(tensorflow)
library(reticulate)	
library(caret)
library(Boruta)
library(kableExtra)
library(finalfit)
library(deepviz)
library(magrittr)
library(corrplot)
library(gridExtra)
```

```{r python, message=FALSE, warning=FALSE, echo=FALSE}
py_path = "C:\\Users\\Tarri\\anaconda3\\" 
use_python(py_path, required = T)	
```


```{r, message=FALSE, warning=FALSE, echo=FALSE}

# url1 <-"https://seagull-erddap.glos.org/erddap/tabledap/obs_22.htmlTable?time%2Clongitude%2Clatitude%2Cchlorophyll_fluorescence%2Cfractional_saturation_of_oxygen_in_sea_water%2Cmass_concentration_of_blue_green_algae_in_sea_water%2Cmass_concentration_of_blue_green_algae_in_sea_water_rfu%2Cmass_concentration_of_chlorophyll_in_sea_water%2Cmass_concentration_of_oxygen_in_sea_water%2Csea_surface_temperature%2Csea_water_electrical_conductivity%2Csea_water_ph_reported_on_total_scale&time%3E=2022-07-25T00%3A00%3A00Z&time%3C=2022-11-01T00%3A00%3A00Z"
# wiki_url1 <- read_html(url1) # UCLA SOCR Data
# df1 <- as.data.frame(html_table(html_nodes(wiki_url1, "table")[[2]]))
# 
# df1 <- df1[-1 , ]
# 
# time <- df1$time
# df1 <- subset(df1, select=-c(time))
# df1 <- as.data.frame(sapply(df1, as.numeric))
# df1 <- cbind(time, df1)
# df1$time <- ymd_hms(df1$time )
# 
# url2 <- "https://seagull-erddap.glos.org/erddap/tabledap/obs_121.htmlTable?time%2Cammonia%2Cphosphate&time%3E=2022-07-25T00%3A00%3A00Z&time%3C=2022-11-01T00%3A00%3A00Z&phosphate!=NaN"
# wiki_url2 <- read_html(url2) # UCLA SOCR Data
# df2 <- as.data.frame(html_table(html_nodes(wiki_url2, "table")[[2]]))
# 
# df2 <- df2[-1 , ]
# 
# time <- df2$time
# df2 <- subset(df2, select=-c(time))
# df2 <- as.data.frame(sapply(df2, as.numeric))
# df2 <- cbind(time, df2)
# df2$time <- ymd_hms(df2$time )
# df2$time <- round_date(df2$time, "10 minutes")
# 
# obs_data <- inner_join(df1, df2, by='time')
# 
# # Pull phycocain flourescence
# url3 <- "https://seagull-erddap.glos.org/erddap/tabledap/obs_117.htmlTable?time%2Cphycocyanin_fluorescence&time%3E=2022-07-21T00%3A00%3A00Z&time%3C=2022-11-01T00%3A00%3A00Z"
# wiki_url3 <- read_html(url3) # UCLA SOCR Data
# df3 <- as.data.frame(html_table(html_nodes(wiki_url3, "table")[[2]]))
# 
# df3 <- df3[-1 , ]
# 
# time <- df3$time
# df3 <- subset(df3, select=-c(time))
# df3 <- as.data.frame(sapply(df3, as.numeric))
# df3 <- cbind(time, df3)
# df3$time <- ymd_hms(df3$time )
# 
# obs_data <- left_join(obs_data, df3, by='time')
# 
# ## Pulls some nitrate data
# url4 <- "https://seagull-erddap.glos.org/erddap/tabledap/obs_163.htmlTable?time%2Cnitrate&time%3E=2022-08-04T00%3A00%3A00Z&time%3C=2022-11-01T00%3A00%3A00Z"
# wiki_url4 <- read_html(url4) # UCLA SOCR Data
# df4 <- as.data.frame(html_table(html_nodes(wiki_url4, "table")[[2]]))
# 
# df4 <- df4[-1 , ]
# 
# time <- df4$time
# df4 <- subset(df4, select=-c(time))
# df4 <- as.data.frame(sapply(df4, as.numeric))
# df4 <- cbind(time, df4)
# df4$time <- ymd_hms(df4$time )
# 
# 
# ## Pulls the rest of the nitrate data
# url5 <- "https://seagull-erddap.glos.org/erddap/tabledap/obs_117.htmlTable?time%2Cnitrate&time%3E=2022-08-04T00%3A00%3A00Z&time%3C=2022-11-01T00%3A00%3A00Z"
# wiki_url5 <- read_html(url5) # UCLA SOCR Data
# df5 <- as.data.frame(html_table(html_nodes(wiki_url5, "table")[[2]]))
# df5 <- df5[-1 , ]
# time <- df5$time
# df5 <- subset(df5, select=-c(time))
# df5 <- as.data.frame(sapply(df5, as.numeric))
# df5 <- cbind(time, df5)
# df5$time <- ymd_hms(df5$time )
# 
# ## Combines the two nitrate dataset to remove the missingness
# df4 <- subset(df4, time > "2022-08-16 18:00:00")
# df5 <- subset(df5, time < "2022-08-29 11:30:00")
# df5 <- df5[1:3650, ]
# nitrate <- rbind(df5, df4)
# 
# ## Combine all data
# obs_data <- left_join(obs_data, nitrate, by='time')

```

```{r data save, message=FALSE, warning=FALSE, echo=FALSE}
#write.csv(obs_data, "C:\\Users\\Tarri\\Desktop\\portfolio_projects\\algal_bloom_dashboard\\data\\algae_data.csv")



```

# Abstract
Harmful algal blooms have been problematic in Lake Erie for over a decade, and have resulted in the reduction in fish population and biodiversity, as well as causing harm to populations residing in nearby areas. The neuro- and hepato- toxins produced by cyanobacteria are difficult to remove in drinking water treatment plants when in high concentrations and can lead to plant shutdown and therefore a reduction of potable water supply for the community. To assist in understanding and reducing bluegreen algae blooms, multiple buoys were deployed on the lake to collect water quality data, along with field sampling and processing done by labs throughout the area. With the long wait times and high costs associated with lab testing, a predictive model was created using deep learning to predict the algae concentration based on the water quality parameters collected by the buoys. This model is able to predict bluegreen algal bloom concentration with 87% accuracy with a slight overfitting causing the predictions to lean towards the more conservative estimate. 


# Background
In the past decade, Lake Erie has seen high concentrations of cyanobacteria, or bluegreen algae. A Severity index was created to rank the algal blooms that occur each year, with the highest severities occuring in 2011 and 2015 with 10 and 10.5 respectively. Not all of the causes of the algal blooms have been determined, however, through research many causes have been identified. These include nutrient-rich water from waste water treatment plants, farm fields and fertilized lawns, invasive species, and warm shallow water in the lake. Furthermore, scientist consider nitrogen in the form of nitrate, and phosphorus to be the main culprit in bluegreen algae growth. (Dean, 2022)

To reduce the risk of harmful algal blooms, the stats of Michigan has planned to focus on reducing phosphorus loads from waste water treatment plants, and agricultural sources in the River Raisin and Maumee River Watersheds. Furthermore, forming collaborative partnerships to provide assistance to farmers and promote conservation practices. Currently local and state focus is on reducing the growth of harmful algae, but implementation of new policy takes time. (Dean, 2022)

To assist in research several buoys were placed in Lake Erie which take multiple water quality parameters that report to research labs throughout the area. Several of these labs also include field sampling data of physicochemical properties along with bluegreen algae concentrations. Using this data, a predictive model can be trained to predict harmful algal bloom concentrations and determine if the concentration is harmful to human and enviromental health.

# Methods
## Data Extraction
Data were pulled into rstudio by reading html tables using the rvest package from the ERDDAP scientific database. This database houses data for water quality parameters provided from buoys, field sampling, and laboratory tests. Data were pulled for the year of 2022, although due to time matching, the data within the time periods from August to November were used. 

The water quality parameters were chosen based on availablity and significance. Some important parameters of note are chlorophyll mass and flourescense, dissolved oxygen saturation mass and fractional, and phycocyanin flourescence. Looking further into these parameters, chlorophyll is used by bluegreen algae to collect photosynthetically active light and therefore may be important in predicting algae concentration (Robert A. Andersen, n.d.). Dissolved oxygen has been known to be depleted during periods of high algal bloom growth which can affect the growth of aquatic plants and animals (Ting-ting Wu, 2015). finally, Phycocyanin is a non-toxic, water-soluble pigment protein from microalgae that exhibits antioxidant, anti-inflammatory, hepatoprotective, and neuroprotective effects (Morais, 2018).

## Missing Data

Since the dataset is about 1000 observations, and the predictive model will require large amounts of data to train, imputation was used rather than removing the columns containing missing data. The Amelia package was used to impute the missing data. The Amelia package imputes data by using the expectation maximization algorithm with bootstrapping. Bootstrapping is a method of inferring results for a population from results found on a collection of smaller random samples of that population, using replacement during the sampling process. This algorithm works by computing the expected value of the log likelihood function with respect to the conditional distribution of Y given X using the parameter estimates of the previous iteration. This is shown as:    
$$Q( \theta | \theta^{(t)} ) = E_{Y | X, \theta^{(t)} }[ log 	\left ( L(\theta | X , Y ) \right ])$$                  
For the maximization step, the expectation is maximized before being used again in the expectation equation. The maximization equations is shown as:     
$$(\theta^{(t+1)}=\arg\max_{\theta}Q(\theta|\theta^{(t)}))$$      

Amelia will create copies of the dataset with new imputed values. The number of copies created will depend on the value for "m" entered. Further analysis is done on each of the "m" datasets so that a variance can be calculated. Distributions are then plotted to compare the original data distribution with the imputed distribution for each of the imputed features to validate the imputation.

## Correlation and Pairs Plots
A correlation and p-value matrix was generated for each feature by using the rcorr() function. The Spearman method was used due to its accuracy in both linear and non linear data. Pairs plots were created for each of the features to access linearity between features. This was done by using the plotly function with a "splom" input.

## Feature Selection
Each features importance was calculated using the Boruta package. Boruta is a feature selection function which utilizes random forest classification to determine the importance of each feature. This is done by first creating "shadow features" by copying and randomizing each original feature before appending to the original dataframe. This gives a dataframe twice the size of the original. Following this, boruta builds a Random Forest Classifier on the new feature space which determines their importance using a statistical test, known as the Z-Score.This algorithm checks if the original feature has higher importance than the maximum importance of shadow features,    
$$(Z-Score_{original} > Z-Score_{Max\, shadow})$$      
If the importance is found to be higher then the feature is recorded as important, otherwise it is recorded as unimportant.

## Normalization
To ensure better optimization in the deep learning neural network, the data was pre-processed by normalization. By normalizing all of the data to values between 0 and 1, the deep learning network will be less likely to get trapped in local extrema caused by highly flucuating values. Instead, the algorithm will have shallower extrema and should be able to converge easier. For normalization, the following function was built,     
$$x_{norm}=\left( \frac{x - min(x))}{(max(x) - min(x)} \right)$$           

## Training and Testing Data
Using the normalized data, training and testing data was created by taking random samples in a 90:10 split. This higher split was used due to using 20% of the training data as a validation step within the neural network. The output variable (bluegreen algae concentration) was left out of the training and testing data, but each (train/test) output was stored for cross validation.

## Deep Learning Neural Network
### Model creation
The Deep learning neural network was built using the python wrapped Keras package. The network consisted of an input layer of 10 nodes (layer 1), followed by a hidden layer of 10 nodes (layer 2), a hidden layer of 120 nodes (layer 3), a dropout layer with a 30% rate (layer 4), and finally a layer with a single output node (layer 5). In each layer, the neural network utilizes an activation function which decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it. Beginning with layer 2, the activation functions for each layer are relu, relu, and linear. The relu function stands for "rectified linear unit" and is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. The linear function, also known as "no activation" is where the activation is proportional to the input and simply returns the value it was given. The dropout layer is used to approximate training a large number of neural networks with different architectures in parallel. During training, a number of layer outputs are randomly ignored which has the effect of making the layer be treated like a layer with a different number of nodes and connectivity to the prior layer. This process attempts to create situations where network layers co-adapt to correct mistakes from prior layers, in turn making the model more robust.

### Model Compiling
The model was compiled using the mean-squared error for both the loss function as well as the metric. The mean-squared error is given by    
$$MSE=\left( \frac{1}{n} \right)\sum_{i=1}^{n}(Y_i-Y'_i)^2$$    
where n is the number of data points, $Y_i$ is the observed value, and $Y'_i$ is the predicted value. This function measures error in statistical models by using the average squared difference between observed and predicted values which tells how close a regression line is to a set of points. Furthermore, Adam was chosen as the optimizer for the model which is a replacement optimization algorithm for stochastic gradient descent. Adam combines properties of the AdaGrad and RMSProp algorithms to create an algorithm that can handle sparse gradients on noisy data.

### Model Fitting
After compliation, the model was fitted using the normalized training data input along with the normalized training data output (bluegreen algae concentration) to be used for validation. A 20% validation split was created from this dataset and validated at each of the 200 epochs. 

### Model Testing
Using the normalized testing data created earlier, the model was evaluated and the correlation to the original data was plotted to show the linearity. The predicted and observed data were then categorized by the danger level of algae (safe, caution, danger) which was decided to be (x<0.6, 0.6<x<1, x>1). These values are based on hazardous levels of the toxins produced by the algae. A confusion matrix was created to determine the accuracy of the classification. 

# Results
## Data Import
The initial data is shown in Table 1. From the table, various summary statistics can be seen such as quantiles, moments, and missing data. This also gives a look at the minimum and maximum values for each column which is useful in determining if the data is within normal bounds.
```{r read in data, message=FALSE, warning=FALSE, echo=FALSE}
algae_data <- read_csv("C:\\Users\\Tarri\\Desktop\\portfolio_projects\\algal_bloom_dashboard\\data\\algae_data.csv")
algae_data[ ,3:17] <- as.data.frame(sapply(algae_data[ ,3:17], abs))

algae_data$nitrate <- algae_data$nitrate/1000
algae_data$ammonia <- algae_data$ammonia/1000
algae_data$phosphate <- algae_data$phosphate/1000

kable(summary(algae_data), "html", caption="Table 1. Initial Water Quality Summary Statistics") %>%
    kable_styling() %>%
    scroll_box(width = "100%", height = "500px")

```


## Missing Data

There appears to be missing data in some of the columns. From Figure 1, we can see that there is only a small amount (~1%) of missing data with the majority of it being in the nitrate data. The columns appear to have a mixture of missing at random and missing not at random data. Since the neural network requires large amounts of data to train, imputation was used rather than removing those observations. However, since some of the data is missing not at random a validation step was added to ensure the distribution of the imputed data followed the original data.

```{r missing data, message=FALSE, warning=FALSE, echo=FALSE}
algae_data[algae_data == 0] <- NA

algae_data %>% missing_plot(title="Figure 1. Missing observations across each feature")
```

The data was imputed with five copies of the data. The variance in the imputed data sets was accounted for after running each copy through the model rather than averaging the data sets together beforehand. Table 2 shows one of the imputed data sets. Note that there are no longer missing values and the mean of the imputed features is still roughly the same (within 5%).

```{r data imputation, message=FALSE, warning=FALSE, echo=FALSE}
## Removes columns that are not needed for the model
algae_sub <- algae_data %>%
  select(-c("...1","time","latitude","longitude"))

## Renaming columns 
headers <- c("chlorophyll_flourescence_rfu","oxygen_saturation_fraction","bluegreen_algae_conc_ug.L",
             "bluegreen_algae_conc_rfu","chlorophyll_conc_kg.m3","oxygen_conc_kg.m3","temp_K","elec_cond_s.m",
             "pH","ammonia_mg.L","phosphate_mg.L", "phycocayanin_flour_rfu","nitrate_mg.L")
names(algae_sub) <- headers

## Convert to dataframe and impute
algae_sub <- as.data.frame(algae_sub)
algae_data1 <- amelia(algae_sub, m=5, p2s=0)

## Create a list of each of the imputed dataframes
algae_list <- list(algae_data1$imputations$imp1, algae_data1$imputations$imp2, algae_data1$imputations$imp3, algae_data1$imputations$imp4, 
                   algae_data1$imputations$imp5)

## Take absolute value of each of the dataframes
for (i in 1:length(algae_list)){
  algae_list[[i]] <- as.data.frame(sapply(algae_list[[i]], abs))
  
  
}

## Summarize the first dataframe
kable(summary(algae_list[[1]]), "html", caption="Table 2. Imputed Water Quality Summary") %>%
    kable_styling() %>%
    scroll_box(width = "100%", height = "500px")
```


To validate the imputation, the density of the imputed data sets and the original data were plotted to assess any differences. Figure 2 shows that for each of the features that required imputation the distribution is almost an exact match. This shows that the imputation was successful so the missing not at random data can still be utilized rather than removed.

```{r checking imputation, warning=FALSE, message=FALSE, echo=FALSE,fig.asp=1.3}
## Create dataframes to append imputed values to
nitrate <- data.frame(nitrate = algae_sub$nitrate_mg.L)
phycoc <- data.frame(phycocayanin = algae_sub$phycocayanin_flour_rfu)
phosphate <- data.frame(phosphate = algae_sub$phosphate_mg.L)
ammonia <- data.frame(ammonia = algae_sub$ammonia_mg.L)

## Appending imputed values to each dataframe
for (df in algae_list){
  nitrate <- cbind(nitrate, df$nitrate_mg.L)
  phycoc <- cbind(phycoc, df$phycocayanin_flour_rfu)
  phosphate <- cbind(phosphate, df$phosphate_mg.L)
  ammonia <- cbind(ammonia, df$ammonia_mg.L)
}

## Creating list of dataframes
imp_list <- list(nitrate, phycoc, phosphate, ammonia)

## Renaming the columns
for (i in 1:length(imp_list)){
  colnames(imp_list[[i]]) <- c("observed", "imp1","imp2","imp3","imp4","imp5")
  }

plot_list <- list()

for (i in 1:length(imp_list)){
## Plotting each dataframe in the list
p <- ggplot(imp_list[[i]])+
  geom_density(aes(x=observed ,fill= "observed" ,alpha=0.7))+
  geom_density(aes(x=imp1, fill = "Imp1"),alpha=0.2)+
  geom_density(aes(x=imp2, fill = "Imp2"),alpha=0.2)+
  geom_density(aes(x=imp3, fill = "Imp3"),alpha=0.2)+
  geom_density(aes(x=imp4, fill = "Imp4"),alpha=0.2)+
  geom_density(aes(x=imp5, fill = "Imp5"),alpha=0.2)+
  xlab("Concentration")+
  ggtitle("Figure 2. Density validation for original and imputed datasets")
  assign(paste("plot",i,sep="_"), ggplotly(p))
}


fig <- plotly::subplot(plot_1, style(plot_2,showlegend=F), style(plot_3,showlegend=F), style(plot_4,showlegend=F), nrows=2,
                       titleY = TRUE, titleX=TRUE,margin = 0.07)
 annotations = list( 
  list( 
    x = 0.22,  
    y = 1,  
    text = "Nitrate",  
    xref = "paper",  
    yref = "paper",  
    xanchor = "center",  
    yanchor = "bottom",  
    showarrow = FALSE 
  ),  
  list( 
    x = 0.78,  
    y = 1,  
    text = "Phycocayanin",  
    xref = "paper",  
    yref = "paper",  
    xanchor = "center",  
    yanchor = "bottom",  
    showarrow = FALSE 
  ),  
  list( 
    x = 0.22,  
    y = 0.42,  
    text = "Phosphate",  
    xref = "paper",  
    yref = "paper",  
    xanchor = "center",  
    yanchor = "bottom",  
    showarrow = FALSE 
  ),
  list( 
    x = 0.78,  
    y = 0.42,  
    text = "Ammonia",  
    xref = "paper",  
    yref = "paper",  
    xanchor = "center",  
    yanchor = "bottom",  
    showarrow = FALSE 
  ))

fig <- fig %>%layout(annotations = annotations) 
fig




```



## Correlation
From literature, it has been found that high levels of nitrogen, phosphorus, and oxygen can cause high levels of algal blooms. However, Figure 3 shows that bluegreen algae concentration only has high correlation with chlorophyll. The high correlation with chlorophyll is likely due to it's use by bluegreen algae when performing photosynthesis. Due to this process, we would expect high levels of chlorophyll when there are high levels of bluegreen algae. Regarding the low nutrient correlations, one possibility could be due to algae ingesting different nitrogen and phosphorus compounds than phosphate, nitrate, and ammonia. If these compounds must be reduced to form compounds which can be injested by the algae, then these compounds would not show correlation to algae concentration in their current form.

```{r correlation, message=FALSE, warning=FALSE, echo=FALSE}


correlation <- rcorr(as.matrix(algae_data1$imputations$imp1, method="Spearman"))


res <- cor(algae_data1$imputations$imp1)

flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}
cor_table <- flattenCorrMatrix(correlation$r, correlation$P)

kable(cor_table, "html", caption="Table 3. Correlation and corresponding P-values") %>%
    kable_styling() %>%
    scroll_box(width = "100%", height = "500px")

corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45,mar=c(0,0,1,0), title = "Figure 3. Correlation plot for water quality parameters")

```

# Pairs Plot
The linearity of each feature was access in Figure 4. The linearity is reflective of the correlation plot showing some linearity between algae concentration and chlorophyll concentration, as well as parameters such as oxygen saturation and temperature. 
```{r pairs plot, message=FALSE, warning=FALSE, echo=FALSE,fig.asp=1.3}
set.seed(42)
pair <- algae_data1$imputations$imp1 %>% select(-c("bluegreen_algae_conc_rfu","oxygen_saturation_fraction","chlorophyll_flourescence_rfu"))
headers <- c("algae_ug.L","chlorophyll_kg.m3","oxygen_kg.m3","temp_K","elec_cond_s.m",
             "pH","ammonia_mg.L","phosphate_mg.L", "phycocayanin_rfu","nitrate_mg.L")

names(pair) <- headers

dims <- dplyr::select_if(pair, is.numeric)
dims <- purrr::map2(dims, names(dims), ~list(values=.x, label=.y))
plot_ly(type = "splom", dimensions = setNames(dims, NULL), showupperhalf = FALSE, 
        diagonal = list(visible = FALSE) ,textangle = 45) %>% 
  layout( title='<b> Figure 4. Lake Erie water parameter Pairs-Plots </b>')

```

## Feature Selection
Due to the low levels of correlation and linearity, Boruta was used to access the importance of each feature in the dataset. Figure 5 shows the results of the Boruta feature selection, which found that all of the features in the dataset are important in the prediction in bluegreen algae growth. However, as found before, chlorophyll is the most important feature in the data set, with ammonia being the least important. Although as stated previously, this may be due to algae using reduced forms of the nutrient compounds. If nutrient data for various forms of each nutrient were available, then the analysis would be more robust.

```{r boruta, message=FALSE, warning=FALSE, echo=FALSE,fig.asp=1.3}
algae_bor <- algae_data1$imputations$imp1 %>%
  select(-c('bluegreen_algae_conc_rfu'))

boruta <- Boruta(bluegreen_algae_conc_ug.L~.,data=algae_bor)

plot(boruta, xlab = "", xaxt = "n", main = "Figure 5. Feature importance")
k <-lapply(1:ncol(boruta$ImpHistory),function(i)
  boruta$ImpHistory[is.finite(boruta$ImpHistory[,i]),i])
names(k) <- colnames(boruta$ImpHistory)
Labels <- sort(sapply(k,median))
axis(side = 1,las=2,labels = names(Labels),
       at = 1:ncol(boruta$ImpHistory), cex.axis = 0.7)


```


# Deep Learning Neural Network
With the data normalized, the training and testing split was made (90:10) and the model was compiled with the Adam optimizer and mean squared error loss function. The results in Figure 6 show that the model predicts the testing data with about 88% accuracy for each of the imputed datasets. Now that each imputed dataset has been fitted to a model for predictions, the predicted values were averaged together and the variance was determined. 

```{r deep learning, message=FALSE, warning=FALSE, echo=FALSE}
 set.seed(55)
i<-0
## Create empty lists to append to in the loop
norm_list_output <- list()
norm_list_input <- list()

## Function to normalize all the columns in the dataset
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}

for (df in algae_list){
  i=i+1
 ## take a subset of the original data which does not include the predicted variable
 ## Also take a subset of just the predicted variable
 dl_all_input <- df %>% select(-c('bluegreen_algae_conc_ug.L','bluegreen_algae_conc_rfu'))
 dl_all_output <- df %>% select(c('bluegreen_algae_conc_ug.L'))

 ## Normalizes both the input and output dataset
 dl_norm_input <-as.data.frame(lapply(dl_all_input, normalize))
 dl_norm_output <-as.data.frame(lapply(dl_all_output, normalize))
 
 ##Append to a appropriate list
 norm_list_output[[i]] <- dl_norm_output
 norm_list_input[[i]] <- dl_norm_input

}
```


```{r train and test, message=FALSE, warning=FALSE, echo=FALSE}
## Create empty lists for appending
train_out_list <- list()
train_in_list <- list()
test_out_list <- list()
test_in_list <- list()

set.seed(55)
## Randomly samples the indexes to create training and testing data
sub <- sample(nrow(dl_norm_output), floor(nrow(dl_norm_output)*0.90))

## This loops through the output datasets, splits them into training and testing data then converts them to a matrix. finally appends them back to a list
i<-0
for (output in norm_list_output){
  i=i+1
 ## Index the input and ouput datasets with the randomly generated indexes to get training and testing inputs/outputs
 dl_train_output <- output[sub, ]
 dl_test_output <- output[-sub, ]

 ## Turn all training and testing data into matrices

 dl_train_output <- as.matrix(dl_train_output)
 dl_test_output <-as.matrix(dl_test_output)
 
 train_out_list[[i]] <- dl_train_output
 test_out_list[[i]] <- dl_test_output
}
i<-0
## This loops through the input datasets, splits them into training and testing data then converts them to a matrix. finally appends them back to a list
for (input in norm_list_input){
  i=i+1
## Index the input and ouput datasets with the randomly generated indexes to get training and testing inputs/outputs
 dl_train_input <- input[sub, ]
 dl_test_input <- input[-sub, ]
 
## Turn all training and testing data into matrices
 dl_train_mat <- as.matrix(dl_train_input)
 dl_test_mat <- as.matrix(dl_test_input)
 
## Remove column names
 colnames(dl_train_mat) <- NULL
 colnames(dl_test_mat) <- NULL
 
 train_in_list[[i]] <- dl_train_mat
 test_in_list[[i]] <- dl_test_mat
}

```



```{r model build, message=FALSE, warning=FALSE, echo=FALSE}
df_list <- list()
plot_list <- list()

dl_model <- keras_model_sequential()
act <- 'relu'
opt <- 'Adam'
loss <- 'mse'
met <- 'mse'
# Add layers to the model
set.seed(55)
dl_model %>%
    layer_dense(units = 10, activation = act, input_shape = 11) %>%
    layer_dense(units = 120, activation = act) %>%
    layer_dropout(rate = 0.3) %>%
    layer_dense(units = 1, activation = 'linear')

dl_model %>% compile(
     loss = loss,
     optimizer = opt,
     metrics = met
 )
summary(dl_model)

for (i in 1:length(train_in_list)){
set.seed(55)
history <- dl_model %>% fit(
  train_in_list[[i]],
  train_out_list[[i]],
  validation_split = 0.2,
  verbose = 0,
  epochs = 200
)


test_results <- dl_model %>% evaluate(
  test_in_list[[i]],
  test_out_list[[i]],
  verbose = 0
)



test_predictions <- predict(dl_model, test_in_list[[i]])

df_list[[i]] <- data.frame(prediction = as.numeric(test_predictions), concentration = test_out_list[[i]])
colnames(df_list[[i]]) <- c("prediction","concentration")

plot_list[[i]] <- plot_ly() %>%
  add_markers(data=df_list[[i]], x=~prediction, y=~concentration,
              name="Data Scatter", type="scatter", mode="markers") %>%
  add_trace(x = c(0,1), y = c(0,1), type="scatter", mode="lines",
        line = list(width = 4), name="Ideal Agreement") %>%
  layout(title="Figure 6. Correlation of observed data and each imputed test dataset",
           xaxis = list(title="NN (hidden=4) Predictions"),
           yaxis = list(title="(Normalized) Observed"),
           legend = list(orientation = 'h'))

cor(df_list[[i]]$prediction, df_list[[i]]$concentration)

#save_model_hdf5(dl_model, paste("C:/Users/Tarri/Desktop/portfolio_projects/algal_bloom_dashboard/code/nn_model_",i,".hdf5", sep=""))

}

```

```{r model testing, warning=FALSE, echo=FALSE, message=FALSE, fig.asp=1.3}

fig <- plotly::subplot(plot_list[[1]], style(plot_list[[2]], showlegend=F),style(plot_list[[3]], showlegend=F),
                       style(plot_list[[4]], showlegend=F), style(plot_list[[5]], showlegend=F), nrows=2,
                       titleY = TRUE, titleX=TRUE,margin=0.08)
 annotations = list( 
  list( 
    x = 0.14,  
    y = 0.97,  
    text = paste0('Cor(Obs,Pred)=',round(cor(df_list[[1]]$prediction, df_list[[1]]$concentration ), 2)),  
    xref = "paper",  
    yref = "paper",  
    xanchor = "center",  
    yanchor = "bottom",  
    showarrow = FALSE 
  ),  
  list( 
    x = 0.50,  
    y = 0.97,  
    text = paste0('Cor(Obs,Pred)=',round(cor(df_list[[2]]$prediction, df_list[[2]]$concentration ), 2)),  
    xref = "paper",  
    yref = "paper",  
    xanchor = "center",  
    yanchor = "bottom",  
    showarrow = FALSE 
  ),  
  list( 
    x = 0.14,  
    y = 0.42,  
    text = paste0('Cor(Obs,Pred)=',round(cor(df_list[[3]]$prediction, df_list[[3]]$concentration ), 2)),  
    xref = "paper",  
    yref = "paper",  
    xanchor = "center",  
    yanchor = "bottom",  
    showarrow = FALSE 
  ),
  list( 
    x = 0.50,  
    y = 0.42,  
    text = paste0('Cor(Obs,Pred)=',round(cor(df_list[[4]]$prediction, df_list[[4]]$concentration ), 2)),  
    xref = "paper",  
    yref = "paper",  
    xanchor = "center",  
    yanchor = "bottom",  
    showarrow = FALSE 
  ),
  list( 
    x = 0.9,  
    y = 0.97,  
    text = paste0('Cor(Obs,Pred)=',round(cor(df_list[[5]]$prediction, df_list[[5]]$concentration ), 2)),  
    xref = "paper",  
    yref = "paper",  
    xanchor = "center",  
    yanchor = "bottom",  
    showarrow = FALSE 
  )  )

fig <- fig %>%layout(annotations = annotations) 
fig
```

Figure 7 shows the averaged predictions along with error bars showing the variance, which are fairly low for each data point. Since the model appears to be over fitting, a confusion matrix was generated to determine if the predictions follow a conservative pattern. Three bins were created based on the safety level of the algae concentration, "safe" refers to concentrations under 0.6 ppb, "caution" for concentrations between 0.6-0.8 ppb, and "danger" for concentrations above 0.8 ppb. 

```{r variance, warning=FALSE, echo=FALSE, message=FALSE }
## Combine the prediction data into one dataframe
var_df <- bind_cols(df_list)

## Take only the predictions
pred_df <- var_df[ , c(1,3,5,7,9)]

##Take only the observations
obs_df <- var_df[ , c(2,4,6,8,10)]

## calculate the variance for the predictions
df <- data.frame(pred_var = apply(pred_df[,-1], 1, var), pred_avg = apply(pred_df[,-1], 1, mean), 
                 obs_var =apply(obs_df[,-1], 1, var), obs_avg =apply(obs_df[,-1], 1, mean))

plot_ly() %>%
  add_markers(data=df, x=~pred_avg, y=~obs_avg, 
              name="Data Scatter", type="scatter", mode="markers",size=2,
              error_x = ~list(array = pred_var,
                        color = 'black')) %>%
  add_trace(x = c(0,1), y = c(0,1), type="scatter", mode="lines",
        line = list(width = 4), name="Ideal Agreement") %>%
  layout(title=paste0('Figure 7. Average Observed vs. Predicted, Cor(Obs,Pred)=',
                      round(cor(df$pred_avg,df$obs_avg ), 2)),
           xaxis = list(title="(Normalized) NN Predictions"),
           yaxis = list(title="(Normalized) Observed"),
           legend = list(orientation = 'h'))
```

As shown in the confusion matrix output, the model is skewed to the more conservative estimate which predicts danger or caution when conditions are actually safe. Although this is not ideal, the conservative estimate is favored over the alternative which would predict safe conditions when it is actually dangerous. Figure 8 shows the distribution of the predicted and observed bluegreen algae concentrations. The predicted results have a higher density around the mean and slightly higher concentration values. Further optimization of the model is need to reduce the over fitting, although around 87% accuracy is still an acceptable result. Future optimization and more data is needed on the deep learning neural network to reach higher accuracy results.

```{r categorical, message=FALSE, warning=FALSE, echo=FALSE}
## Function to return concentration to its pre normalized value
unnormalize <- function(x) {
return((x*(max(dl_all_output$bluegreen_algae_conc_ug.L)-min(dl_all_output$bluegreen_algae_conc_ug.L))+min(dl_all_output$bluegreen_algae_conc_ug.L)))
}

## Create a data frame for new prediction values
pred <-as.data.frame(lapply(df, unnormalize))

pred$warning_pred <- ifelse(pred$pred_avg > 1, "Danger", ifelse( (pred$pred_avg > 0.6 & pred$pred_avg <= 1), "Caution", "Safe"))
pred$warning_obs <- ifelse(pred$obs_avg > 1, "Danger", ifelse( (pred$obs_avg > 0.6 & pred$obs_avg <= 1), "Caution", "Safe"))

p <- ggplot(pred) +
  geom_density(aes(x=pred_avg, fill="predicted"), alpha=0.5)+
  geom_density(aes(x=obs_avg ,fill="Observed"), alpha=0.5)+
  labs(x="Concentration of bluegreen algae (ug/L)", title = "Figure 8. Density of observed concentration vs. predicted concentration")

confusionMatrix(data=as.factor(pred$warning_pred), reference = as.factor(pred$warning_obs))

ggplotly(p)


```

# Conclusion
To combat the growth of harmful bluegreen algal blooms, multiple water quality parameters are being taken in real time along with field sampling of aqueous nutrients. To further assist in this, a model was trained to predict bluegreen algae concentrations by using the water quality data available from the buoys, as well as the nutrient sampling done in lab. Prior to model training, the data were imputed with 5 copies to remove missing values, which is reflected in the variance in the predicted values. Using a correlation matrix, it was found that chlorophyll was highly correlated with bluegreen algae concentration, but no other parameters were closely correlated. However, this could be due to other reactions happening with other parameters not shown. To access the importance of each feature, Boruta was used to ensure that the features being input into the model will not negatively impacts the weights and bias. Boruta determined that all of the features were important in determining bluegreen algae concentration. 

The model was created with two hidden layers, on with 10 nodes, and one with 120 nodes and a layer dropout rate of 0.3. The Adam optimizer was used along with the mean squared error as the loss function and metric. Relu was used for the activation function in both hidden layers, with a linear activation function for the output. The model was evaulated using a training dataset created from a 90:10 split along with a 20% validation. Fitting the model to the testing data resulted in a 87% correlation between the predicted and observed values. Since the model was overfitting slightly, bins were created for safety levels, less than 0.6 is safe, between 0.6 and 0.8 is caution, and greater than 0.8 is danger. The confusion matrix determined that the model was in fact overfitting which caused slightly more conservative estimates, which in the case of public safety is the preferred estimate. Further optimization is needed on the deep learning neural network to increase accuracy beyond 90%. Furthermore, more data are needed across larger ranges of values to create a more robust model.

# Acknowledgements
I would like to thank Professor Dinov for his guidance throughout this project. I would also like to thank the labs who allowed their data to be open source and easily available.

# References
Brownlee, J. (2019, Jan 09). A Gentle Introduction to the Rectified Linear Unit (ReLU). Retrieved from Machine Learning Mastery: https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/

Dean, S. (2022, August 11). Harmful algal blooms in Lake Erie expected to be smaller this year, says NOAA. Retrieved from Michigan.gov: https://www.michigan.gov/egle/newsroom/mi-environment/2022/08/11/harmful-algal-blooms-in-lake-erie-expected-to-be-smaller-this-year-says-noaa#:~:text=In%20Lake%20Erie%2C%20several%20factors,aren't%20quite%20understood%20yet.

Morais, M. G. (2018, Geb 1). Phycocyanin from Microalgae: Properties, Extraction and Purification, with Some Recent Applications. Retrieved from https://www.liebertpub.com/doi/10.1089/ind.2017.0009#:~:text=Phycocyanin%20is%20a%20non%2Dtoxic,%2C%20hepatoprotective%2C%20and%20neuroprotective%20effects.

Robert A. Andersen, R. A. (n.d.). Photosynthesis and light-absorbing pigments. Retrieved from Britannica: https://www.britannica.com/science/algae/Photosynthesis-and-light-absorbing-pigments

Seagull. (n.d.). Retrieved from Seagull: https://seagull.glos.org/map?coords=-83.4103060%2C41.6266502%2C10&lake=Erie&tags=platforms%3Abuoy%2Cweather%3A%2Cwater%3A%2Cfavorite%3A&platform=RBS-TOL

Ting-ting Wu, G.-f. L.-q.-y. (2015, Jan). Impacts of algal blooms accumulation on physiological ecology of water hyacinth. Retrieved from National Library of Midicine: https://pubmed.ncbi.nlm.nih.gov/25898654/


