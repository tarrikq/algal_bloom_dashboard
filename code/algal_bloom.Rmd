---
title: "Final Report"
subtitle: "<h2><u>Data Science and Predictive Analytics (HS650)</u></h2>"
author: "<h3>Tarrik Quneibi</h3>"
date: "`r format(Sys.time(), '%B %Y')`"
output:
  html_document:
    theme: journal
    highlight: tango
    toc: true
    number_sections: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_folding: show
    self_contained: yes
---

 * Fall 2022, DSPA (HS650)
 * SID: 1236 
 * UMich E-mail: tarrikq@umich.edu
 * I certify that the following paper represents my own independent work and conforms with the guidelines of academic honesty described in the     UMich student handbook.
 * Remember that students are allowed and encouraged to discuss, on a conceptual level, the problems with your class mates, however, this can      not involve the exchange of actual code, printouts, solutions, e-mails or other explicit electronic or paper handouts.

Outline of problem
 * Include the regular HW project cover page. Start with a one paragraph abstract, followed by an intro/background of the problem, methods, results, discussion/conclusion and acknowledgments, references, in that order. Clearly state the problem you have chosen to investigate. List the resources you used to come up with the project and reference all sources you used to complete the project.
 * Clearly state your hypotheses, prior to interrogating the data.
 * Use statistical techniques from the list of techniques we have discussed in the course to convey whether or not there is statistical evidence in support of your original hypotheses.
 * Explicitly state your approach to answer your research hypotheses. Write all formulas/tests/statistics you need.
 * Interpret your statistical (numerical) results in a lay back language. Write conclusions and discussions at the end of your report and acknowledge outside help.  *  * Describe how this project can be extended in the future.
 * One, two or three people can work on a project as a team. If people team up, everyone must contribute equally to the project and all members must submit separate copies of the project, with their names on top (the names of all team members should be on all submissions). Expectations of team projects are higher.
 * It's strongly recommended that you design your study, implement your analytic protocol, conduct that modeling, package and report the findings using RMD e-notebook.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message=FALSE, warning=FALSE, echo=FALSE}
library(readr)
library(rvest)
library(plotly)
library(ggplot2)
library(lubridate)
library(Amelia)
library(tidyverse)
library(Hmisc)
library(neuralnet)
library(keras)
library(tensorflow)
library(reticulate)	
library(caret)
library(Boruta)
library(kableExtra)
library(finalfit)
library(deepviz)
library(magrittr)
```

```{r python, message=FALSE, warning=FALSE, echo=FALSE}
py_path = "C:\\Users\\Tarri\\anaconda3\\" 
use_python(py_path, required = T)	
```


```{r, message=FALSE, warning=FALSE, echo=FALSE}

# url1 <-"https://seagull-erddap.glos.org/erddap/tabledap/obs_22.htmlTable?time%2Clongitude%2Clatitude%2Cchlorophyll_fluorescence%2Cfractional_saturation_of_oxygen_in_sea_water%2Cmass_concentration_of_blue_green_algae_in_sea_water%2Cmass_concentration_of_blue_green_algae_in_sea_water_rfu%2Cmass_concentration_of_chlorophyll_in_sea_water%2Cmass_concentration_of_oxygen_in_sea_water%2Csea_surface_temperature%2Csea_water_electrical_conductivity%2Csea_water_ph_reported_on_total_scale&time%3E=2022-07-25T00%3A00%3A00Z&time%3C=2022-11-01T00%3A00%3A00Z"
# wiki_url1 <- read_html(url1) # UCLA SOCR Data
# df1 <- as.data.frame(html_table(html_nodes(wiki_url1, "table")[[2]]))
# 
# df1 <- df1[-1 , ]
# 
# time <- df1$time
# df1 <- subset(df1, select=-c(time))
# df1 <- as.data.frame(sapply(df1, as.numeric))
# df1 <- cbind(time, df1)
# df1$time <- ymd_hms(df1$time )
# 
# url2 <- "https://seagull-erddap.glos.org/erddap/tabledap/obs_121.htmlTable?time%2Cammonia%2Cphosphate&time%3E=2022-07-25T00%3A00%3A00Z&time%3C=2022-11-01T00%3A00%3A00Z&phosphate!=NaN"
# wiki_url2 <- read_html(url2) # UCLA SOCR Data
# df2 <- as.data.frame(html_table(html_nodes(wiki_url2, "table")[[2]]))
# 
# df2 <- df2[-1 , ]
# 
# time <- df2$time
# df2 <- subset(df2, select=-c(time))
# df2 <- as.data.frame(sapply(df2, as.numeric))
# df2 <- cbind(time, df2)
# df2$time <- ymd_hms(df2$time )
# df2$time <- round_date(df2$time, "10 minutes")
# 
# obs_data <- inner_join(df1, df2, by='time')
# 
# # Pull phycocain flourescence
# url3 <- "https://seagull-erddap.glos.org/erddap/tabledap/obs_117.htmlTable?time%2Cphycocyanin_fluorescence&time%3E=2022-07-21T00%3A00%3A00Z&time%3C=2022-11-01T00%3A00%3A00Z"
# wiki_url3 <- read_html(url3) # UCLA SOCR Data
# df3 <- as.data.frame(html_table(html_nodes(wiki_url3, "table")[[2]]))
# 
# df3 <- df3[-1 , ]
# 
# time <- df3$time
# df3 <- subset(df3, select=-c(time))
# df3 <- as.data.frame(sapply(df3, as.numeric))
# df3 <- cbind(time, df3)
# df3$time <- ymd_hms(df3$time )
# 
# obs_data <- left_join(obs_data, df3, by='time')
# 
# ## Pulls some nitrate data
# url4 <- "https://seagull-erddap.glos.org/erddap/tabledap/obs_163.htmlTable?time%2Cnitrate&time%3E=2022-08-04T00%3A00%3A00Z&time%3C=2022-11-01T00%3A00%3A00Z"
# wiki_url4 <- read_html(url4) # UCLA SOCR Data
# df4 <- as.data.frame(html_table(html_nodes(wiki_url4, "table")[[2]]))
# 
# df4 <- df4[-1 , ]
# 
# time <- df4$time
# df4 <- subset(df4, select=-c(time))
# df4 <- as.data.frame(sapply(df4, as.numeric))
# df4 <- cbind(time, df4)
# df4$time <- ymd_hms(df4$time )
# 
# 
# ## Pulls the rest of the nitrate data
# url5 <- "https://seagull-erddap.glos.org/erddap/tabledap/obs_117.htmlTable?time%2Cnitrate&time%3E=2022-08-04T00%3A00%3A00Z&time%3C=2022-11-01T00%3A00%3A00Z"
# wiki_url5 <- read_html(url5) # UCLA SOCR Data
# df5 <- as.data.frame(html_table(html_nodes(wiki_url5, "table")[[2]]))
# df5 <- df5[-1 , ]
# time <- df5$time
# df5 <- subset(df5, select=-c(time))
# df5 <- as.data.frame(sapply(df5, as.numeric))
# df5 <- cbind(time, df5)
# df5$time <- ymd_hms(df5$time )
# 
# ## Combines the two nitrate dataset to remove the missingness
# df4 <- subset(df4, time > "2022-08-16 18:00:00")
# df5 <- subset(df5, time < "2022-08-29 11:30:00")
# df5 <- df5[1:3650, ]
# nitrate <- rbind(df5, df4)
# 
# ## Combine all data
# obs_data <- left_join(obs_data, nitrate, by='time')

```

```{r data save, message=FALSE, warning=FALSE, echo=FALSE}
#write.csv(obs_data, "C:\\Users\\Tarri\\Desktop\\portfolio_projects\\algal_bloom_dashboard\\data\\algae_data.csv")



```

# Abstract

# Background
In the past decade, Lake Erie has seen high concentrations of cyanobactera, or bluegreen algae. A Severity index was created to rank the algal blooms that occur each year, with the highest severities occuring in 2011 and 2015 with 10 and 10.5 respectively. Not all of the causes of the algal blooms have been determined, however, through research many causes have been identified. These include nutrient-rich water from waste water treatment plants, farm fields and fertilized lawns, invasive species, and warm shallow water in the lake. Furthermore, scientist consider nitrogen in the form of nitrate, and phosphorus to be the main culprit in bluegreen algae growth. (Dean, 2022)

To reduce the risk of harmful algal blooms, the stats of Michigan has planned to focus on reducing phosphorus loads from waste water treatment plants, and agricultural sources in the River Raisin and Maumee River Watersheds. Furthermore, forming collaborative partnerships to provide assistance to farmers and promote conservation practices. Currently local and state focus is on reducing the growth of harmful algae, but implementation of new policy takes time. (Dean, 2022)

To assist in research several buoys were placed in Lake Erie which take multiple water quality parameters that report to research labs throughout the area. Several of these labs also include field sampling data of physicochemical properties along with bluegreen algae concentrations. Using this data, a predictive model can be trained to predict harmful algal bloom concentrations and determine if the concentration is harmful to human and enviromental health.

# Methods
## Data Extraction
Data were pulled into rstudio by reading html tables using the rvest package from the ERDDAP scientific database. This database houses data for water quality parameters provided from buoys, field sampling, and laboratory tests. Data were pulled for the year of 2022, although due to time matching, the data within the time periods from August to November were used. 

The water quality parameters were chosen based on availablity and significance. Some important parameters of note are chlorophyll mass and flourescense, dissolved oxygen saturation mass and fractional, and phycocyanin flourescence. Looking further into these parameters, chlorophyll is used by bluegreen algae to collect photosynthetically active light and therefore may be important in predicting algae concentration (Robert A. Andersen, n.d.). Dissolved oxygen has been known to be depleted during periods of high algal bloom growth which can affect the growth of aquatic plants and animals (Ting-ting Wu, 2015). finally, Phycocyanin is a non-toxic, water-soluble pigment protein from microalgae that exhibits antioxidant, anti-inflammatory, hepatoprotective, and neuroprotective effects (Morais, 2018).

## Missing Data

Since the dataset is about 1000 observations, and the predictive model will require large amounts of data to train, imputation was used rather than removing the columns containing missing data. The Amelia package was used to impute the missing data. The Amelia package imputes data by using the expectation maximization algorithm with bootstrapping. Bootstrapping is a method of inferring results for a population from results found on a collection of smaller random samples of that population, using replacement during the sampling process. This algorithm works by computing the expected value of the log likelihood function with respect to the conditional distribution of Y given X using the parameter estimates of the previous iteration. This is shown as:
 $Q( \theta | \theta^{(t)} ) = E_{Y | X, \theta^{(t)} }[ log 	\left ( L(\theta | X , Y ) \right ])$
For the maximization step, the expectation is maximized before being used again in the expectation equation. The maximization equations is shown as:
 $(\theta^{(t+1)}=\arg\max_{\theta}Q(\theta|\theta^{(t)}).)$

Amelia will create copies of the dataset with new imputed values. The number of copies created will depend on the value for "m" entered. Further analysis is done on each of the "m" datasets so that a variance can be calculated. Distributions are then plotted to compare the original data distribution with the imputed distribution for each of the imputed features to validate the imputation.

## Correlation and Pairs Plots
A correlation and p-value matrix was generated for each feature by using the rcorr() function. The Spearman method was used due to its accuracy in both linear and non linear data. Pairs plots were created for each of the features to access linearity between features. This was done by using the plotly function with a "splom" input.

## Feature Selection
Each features importance was calculated using the Boruta package. Boruta is a feature selection function which utilizes random forest classification to determine the importance of each feature. This is done by first creating "shadow features" by copying and randomizing each original feature before appending to the original dataframe. This gives a dataframe twice the size of the original. Following this, boruta builds a Random Forest Classifier on the new feature space which determines their importance using a statistical test, known as the Z-Score.This algorithm checks if the original feature has higher importance than the maximum importance of shadow features, $(Z-Score_{original} > Z-Score_{Max\, shadow})$. If the importance is found to be higher then the feature is recorded as important, otherwise it is recorded as unimportant.

## Normalization
To ensure better optimization in the deep learning neural network, the data was pre-processed by normalization. By normalizing all of the data to values between 0 and 1, the deep learning network will be less likely to get trapped in local extrema caused by highly flucuating values. Instead, the algorithm will have shallower extrema and should be able to converge easier. For normalization, the following function was built, $x_{norm}=\left( \frac{x - min(x))}{(max(x) - min(x)} \right)$.

## Training and Testing Data
Using the normalized data, training and testing data was created by taking random samples in a 90:10 split. This higher split was used due to using 20% of the training data as a validation step within the neural network. The output variable (bluegreen algae concentration) was left out of the training and testing data, but each (train/test) output was stored for cross validation.

## Deep Learning Neural Network
### Model creation
The Deep learning neural network was built using the python wrapped Keras package. The network consisted of an input layer of 10 nodes (layer 1), followed by a hidden layer of 10 nodes (layer 2), a hidden layer of 120 nodes (layer 3), a dropout layer with a 30% rate (layer 4), and finally a layer with a single output node (layer 5). In each layer, the neural network utilizes an activation function which decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it. Beginning with layer 2, the activation functions for each layer are relu, relu, and linear. The relu function stands for "rectified linear unit" and is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. The linear function, also known as "no activation" is where the activation is proportional to the input and simply returns the value it was given. The dropout layer is used to approximate training a large number of neural networks with different architectures in parallel. During training, a number of layer outputs are randomly ignored which has the effect of making the layer be treated like a layer with a different number of nodes and connectivity to the prior layer. This process attempts to create situations where network layers co-adapt to correct mistakes from prior layers, in turn making the model more robust.

### Model Compiling
The model was compiled using the mean-squared error for both the loss function as well as the metric. The mean-squared error is given by $MSE=\left( \frac{1}{n} \right)\sum_{i=1}^{n}(Y_i-Y'_i)^2$, where n is the number of data points, $Y_i$ is the observed value, and $Y'_i$ is the predicted value. This function measures error in statistical models by using the average squared difference between observed and predicted values which tells how close a regression line is to a set of points. Furthermore, Adam was chosen as the optimizer for the model which is a replacement optimization algorithm for stochastic gradient descent. Adam combines properties of the AdaGrad and RMSProp algorithms to create an algorithm that can handle sparse gradients on noisy data.

### Model Fitting
After compliation, the model was fitted using the normalized training data input along with the normalized training data output (bluegreen algae concentration) to be used for validation. A 20% validation split was created from this dataset and validated at each of the 200 epochs. 

### Model Testing
Using the normalized testing data created earlier, the model was evaluated and the correlation to the original data was plotted to show the linearity. The predicted and observed data were then categorized by the danger level of algae (safe, caution, danger) which was decided to be (x<0.6, 0.6<x<1, x>1). These values are based on hazardous levels of the toxins produced by the algae. A confusion matrix was created to determine the accuracy of the classification. 

# Results

```{r read in data, message=FALSE, warning=FALSE, echo=FALSE}
algae_data <- read_csv("C:\\Users\\Tarri\\Desktop\\portfolio_projects\\algal_bloom_dashboard\\data\\algae_data.csv")
algae_data[ ,3:17] <- as.data.frame(sapply(algae_data[ ,3:17], abs))

kable(summary(algae_data), "html", caption="Table 1. Initial Water Quality Summary Statistics") %>%
    kable_styling() %>%
    scroll_box(width = "100%", height = "500px")

```



From Table 1, there appears to be missing data in three of the columns. From the missingness map, we can see that there is only a small amount of missing data with the majority of it being in the nitrate data. To train a predictive model the missing data will need to be either removed or imputed. 

```{r missing data, message=FALSE, warning=FALSE, echo=FALSE}
algae_data[algae_data == 0] <- NA

algae_data %>% missing_plot()
```

```{r data imputation, message=FALSE, warning=FALSE, echo=FALSE}

algae_sub <- algae_data %>%
  select(-c("...1","time","latitude","longitude"))
headers <- c("chlorophyll_flourescence_rfu","oxygen_saturation_fraction","bluegreen_algae_conc_ug.L",
             "bluegreen_algae_conc_rfu","chlorophyll_conc_kg.m3","oxygen_conc_kg.m3","temp_K","elec_cond_s.m",
             "pH","ammonia_mg.L","phosphate_mg.L", "phycocayanin_flour_rfu","nitrate_mg.L")

names(algae_sub) <- headers
algae_sub <- as.data.frame(algae_sub)

algae_sub$nitrate_mg.L <- algae_sub$nitrate_mg.L/1000
algae_sub$ammonia_mg.L <- algae_sub$ammonia_mg.L/1000
algae_sub$phosphate_mg.L <- algae_sub$phosphate_mg.L/1000

algae_data1 <- amelia(algae_sub, m=5, p2s=0)

algae_sub <- (algae_data1$imputations$imp1+algae_data1$imputations$imp2+algae_data1$imputations$imp3+algae_data1$imputations$imp4+algae_data1$imputations$imp5)/5
algae_sub <- as.data.frame(sapply(algae_sub, abs))



kable(summary(algae_sub), "html", caption="Table 2. Imputed Water Quality Summary") %>%
    kable_styling() %>%
    scroll_box(width = "100%", height = "500px")
```


```{r checking imputation}
imp_list <- list(
  data.frame(initial = algae_data$nitrate/1000, imputed = algae_sub$nitrate_mg.L),
  data.frame(initial = algae_data$phycocyanin_fluorescence, imputed = algae_sub$phycocayanin_flour_rfu),
  data.frame(initial = algae_data$phosphate/1000, imputed = algae_sub$phosphate_mg.L),
  data.frame(initial = algae_data$ammonia/1000, imputed = algae_sub$ammonia_mg.L)

)

names <- c("nitrate","phycocyanin","phosphate","ammonia")

plotting <- function(df){
p<- ggplot(df)+
  geom_density(aes(x=initial, fill = "Initial"),alpha=0.7)+
  geom_density(aes(x=imputed, fill = "Imputed"),alpha=0.5)+
  labs(x="Concentration")
ggplotly(p)
}

lapply(imp_list, plotting)
```

# Correlation

```{r correlation, message=FALSE, warning=FALSE, echo=FALSE}


correlation <- rcorr(as.matrix(algae_sub, method="Spearman"))


```

# Pairs Plot

```{r pairs plot, message=FALSE, warning=FALSE, echo=FALSE}
set.seed(42)

dims <- dplyr::select_if(algae_sub, is.numeric)
dims <- purrr::map2(dims, names(dims), ~list(values=.x, label=.y))
plot_ly(type = "splom", dimensions = setNames(dims, NULL), showupperhalf = FALSE, 
        diagonal = list(visible = FALSE) ,textangle = 45) %>% 
  layout( title='<b> Lake Erie water parameter Pairs-Plots </b>')

```

```{r boxplot}

box <- algae_sub %>% select(c("nitrate_mg.L","phosphate_mg.L","ammonia_mg.L"))

ggplot(stack(box), aes(x = ind, y = values)) +
  geom_boxplot()

```

# Feature Selection
```{r boruta, message=FALSE, warning=FALSE, echo=FALSE}
algae_bor <- algae_sub %>%
  select(-c('bluegreen_algae_conc_rfu'))

boruta <- Boruta(bluegreen_algae_conc_ug.L~.,data=algae_bor)

plot(boruta, xlab = "", xaxt = "n")
k <-lapply(1:ncol(boruta$ImpHistory),function(i)
  boruta$ImpHistory[is.finite(boruta$ImpHistory[,i]),i])
names(k) <- colnames(boruta$ImpHistory)
Labels <- sort(sapply(k,median))
axis(side = 1,las=2,labels = names(Labels),
       at = 1:ncol(boruta$ImpHistory), cex.axis = 0.7)


```


# Neural Network

```{r deep learning, message=FALSE, warning=FALSE, echo=FALSE}
 set.seed(55)
 ## take a subset of the original data which does not include the predicted variable
 ## Also take a subset of just the predicted variable
 dl_all_input <- algae_sub %>% select(-c('bluegreen_algae_conc_ug.L','bluegreen_algae_conc_rfu'))
 dl_all_output <- algae_sub %>% select(c('bluegreen_algae_conc_ug.L'))

 ## Function to normalize all the columns in the dataset
 normalize <- function(x) {
 return((x - min(x)) / (max(x) - min(x)))
 }

 ## Normalizes both the input and output dataset
 dl_norm_input <-as.data.frame(lapply(dl_all_input, normalize))
 dl_norm_output <-as.data.frame(lapply(dl_all_output, normalize))
 
```


```{r train and test}
 ## Randomly samples the indexes to create training and testing data
 sub <- sample(nrow(dl_norm_output), floor(nrow(dl_norm_output)*0.90))

 ## Index the input and ouput datasets with the randomly generated indexes to get training and testing inputs/outputs
 dl_train_input <- dl_norm_input[sub, ]
 dl_train_output <-dl_norm_output[sub, ]
 dl_test_input <- dl_norm_input[-sub, ]
 dl_test_output <-dl_norm_output[-sub, ]


 ## Turn all training and testing data into matrices
 dl_train_mat <- as.matrix(dl_train_input)
 dl_test_mat <- as.matrix(dl_test_input)
 dl_train_output <- as.matrix(dl_train_output)
 dl_test_output <-as.matrix(dl_test_output)

 ## Remove column names
 colnames(dl_train_mat) <- NULL
 colnames(dl_test_mat) <- NULL



```



```{r model build, message=FALSE, warning=FALSE, echo=FALSE}
# dl_model <- keras_model_sequential()
# act <- 'relu'
# opt <- 'Adam'
# loss <- 'mse'
# met <- 'mse'
# # Add layers to the model
# set.seed(55)
# dl_model %>%
#     layer_dense(units = 10, activation = act, input_shape = length(dl_train_input)) %>%
#     layer_dense(units = 120, activation = act) %>%
#     layer_dropout(rate = 0.3) %>%
#     layer_dense(units = 1, activation = 'linear')
# 
# dl_model %>% compile(
#      loss = loss,
#      optimizer = opt,
#      metrics = met
#  )
# summary(dl_model)
# 
# history <- dl_model %>% fit(
#   dl_train_mat,
#   dl_train_output,
#   validation_split = 0.2,
#   verbose = 0,
#   epochs = 200
# )
# 
# plot(history)
# 
# 
# test_results <- dl_model %>% evaluate(
#   dl_test_mat,
#   dl_test_output,
#   verbose = 0
# )
# test_results
# 
# 
# test_predictions <- predict(dl_model, dl_test_mat)
# 
# df <- data.frame(prediction = as.numeric(test_predictions), concentration = dl_test_output)
# colnames(df) <- c("prediction","concentration")
# plot_ly() %>%
#   add_markers(data=df, x=~prediction, y=~concentration,
#               name="Data Scatter", type="scatter", mode="markers") %>%
#   add_trace(x = c(0,1), y = c(0,1), type="scatter", mode="lines",
#         line = list(width = 4), name="Ideal Agreement") %>%
#   layout(title=paste0('Scatterplot (Normalized) Observed vs. Predicted Values, Cor(Obs,Pred)=',
#                       round(cor(df$prediction,df$concentration ), 2)),
#            xaxis = list(title="NN (hidden=4) Predictions"),
#            yaxis = list(title="(Normalized) Observed"),
#            legend = list(orientation = 'h'))
# 
# cor(df$prediction, df$concentration)


```

```{r model save, message=FALSE, warning=FALSE, echo=FALSE}
#save_model_hdf5(dl_model, "C:/Users/Tarri/Desktop/portfolio_projects/algal_bloom_dashboard/code/nn_model.hdf5")
```



```{r NN , message=FALSE, warning=FALSE, echo=FALSE}
model = load_model_hdf5('C:/Users/Tarri/Desktop/portfolio_projects/algal_bloom_dashboard/code/nn_model.hdf5')
```

```{r , message=FALSE, warning=FALSE, echo=FALSE}

test_results <- model %>% evaluate(
  dl_test_mat,
  dl_test_output,
  verbose = 0
)
test_results


test_predictions <- predict(model, dl_test_mat)

df <- data.frame(prediction = as.numeric(test_predictions), concentration = dl_test_output)

plot_ly() %>%
  add_markers(data=df, x=~prediction, y=~concentration, 
              name="Data Scatter", type="scatter", mode="markers") %>%
  add_trace(x = c(0,1), y = c(0,1), type="scatter", mode="lines",
        line = list(width = 4), name="Ideal Agreement") %>%
  layout(title=paste0('Scatterplot (Normalized) Observed vs. Predicted Values, Cor(Obs,Pred)=',
                      round(cor(df$prediction,df$concentration ), 2)),
           xaxis = list(title="(Normalized) NN Predictions"),
           yaxis = list(title="(Normalized) Observed"),
           legend = list(orientation = 'h'))
```

```{r categorical, message=FALSE, warning=FALSE, echo=FALSE}
## Function to return concentration to its pre normalized value
unnormalize <- function(x) {
return((x*(max(dl_all_output$bluegreen_algae_conc_ug.L)-min(dl_all_output$bluegreen_algae_conc_ug.L))+min(dl_all_output$bluegreen_algae_conc_ug.L)))
}

## Create a data frame for new prediction values
pred <-as.data.frame(lapply(df, unnormalize))

pred$warning_pred <- ifelse(pred$prediction > 1, "Danger", ifelse( (pred$prediction > 0.6 & pred$prediction <= 1), "Caution", "Safe"))
pred$warning_obs <- ifelse(pred$concentration > 1, "Danger", ifelse( (pred$concentration > 0.6 & pred$concentration <= 1), "Caution", "Safe"))

p <- ggplot(pred) +
  geom_density(aes(x=prediction, fill="Predicted"), alpha=0.5)+
  geom_density(aes(x=concentration,fill="Observed"), alpha=0.5)+
  labs(x="Concentration of bluegreen algae (ug/L)", title = "Observed concentration vs. predicted concentration")

table(pred[3:4])
ggplotly(p)

plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

```

# References

