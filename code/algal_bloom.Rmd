---
title: "algal_bloom"
author: "Tarrik Quneibi"
date: "2022-11-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(readr)
library(rvest)
library(plotly)
library(ggplot2)
library(lubridate)
library(Amelia)
library(tidyverse)
library(Hmisc)
library(neuralnet)
library("keras")
library(tensorflow)
library(reticulate)	
library(caret)
library(Boruta)
```
```{r python}
py_path = "C:\\Users\\Tarri\\anaconda3\\" 
use_python(py_path, required = T)	
```


```{r}

# url1 <-"https://seagull-erddap.glos.org/erddap/tabledap/obs_22.htmlTable?time%2Clongitude%2Clatitude%2Cchlorophyll_fluorescence%2Cfractional_saturation_of_oxygen_in_sea_water%2Cmass_concentration_of_blue_green_algae_in_sea_water%2Cmass_concentration_of_blue_green_algae_in_sea_water_rfu%2Cmass_concentration_of_chlorophyll_in_sea_water%2Cmass_concentration_of_oxygen_in_sea_water%2Csea_surface_temperature%2Csea_water_electrical_conductivity%2Csea_water_ph_reported_on_total_scale&time%3E=2022-07-25T00%3A00%3A00Z&time%3C=2022-11-01T00%3A00%3A00Z"
# wiki_url1 <- read_html(url1) # UCLA SOCR Data
# df1 <- as.data.frame(html_table(html_nodes(wiki_url1, "table")[[2]]))
# 
# df1 <- df1[-1 , ]
# 
# time <- df1$time
# df1 <- subset(df1, select=-c(time))
# df1 <- as.data.frame(sapply(df1, as.numeric))
# df1 <- cbind(time, df1)
# df1$time <- ymd_hms(df1$time )
# 
# url2 <- "https://seagull-erddap.glos.org/erddap/tabledap/obs_121.htmlTable?time%2Cammonia%2Cphosphate&time%3E=2022-07-25T00%3A00%3A00Z&time%3C=2022-11-01T00%3A00%3A00Z&phosphate!=NaN"
# wiki_url2 <- read_html(url2) # UCLA SOCR Data
# df2 <- as.data.frame(html_table(html_nodes(wiki_url2, "table")[[2]]))
# 
# df2 <- df2[-1 , ]
# 
# time <- df2$time
# df2 <- subset(df2, select=-c(time))
# df2 <- as.data.frame(sapply(df2, as.numeric))
# df2 <- cbind(time, df2)
# df2$time <- ymd_hms(df2$time )
# df2$time <- round_date(df2$time, "10 minutes")
# 
# obs_data <- inner_join(df1, df2, by='time')
# 
# url3 <- "https://seagull-erddap.glos.org/erddap/tabledap/obs_117.htmlTable?time%2Cphycocyanin_fluorescence&time%3E=2022-07-21T00%3A00%3A00Z&time%3C=2022-11-01T00%3A00%3A00Z"
# wiki_url3 <- read_html(url3) # UCLA SOCR Data
# df3 <- as.data.frame(html_table(html_nodes(wiki_url3, "table")[[2]]))
# 
# df3 <- df3[-1 , ]
# 
# time <- df3$time
# df3 <- subset(df3, select=-c(time))
# df3 <- as.data.frame(sapply(df3, as.numeric))
# df3 <- cbind(time, df3)
# df3$time <- ymd_hms(df3$time )
# 
# obs_data <- left_join(obs_data, df3, by='time')
# 
# url4 <- "https://seagull-erddap.glos.org/erddap/tabledap/obs_162.htmlTable?time%2Cnitrate&time%3E=2022-07-21T00%3A00%3A00Z"
# wiki_url4 <- read_html(url4) # UCLA SOCR Data
# df4 <- as.data.frame(html_table(html_nodes(wiki_url4, "table")[[2]]))
# 
# df4 <- df4[-1 , ]
# 
# time <- df4$time
# df4 <- subset(df4, select=-c(time))
# df4 <- as.data.frame(sapply(df4, as.numeric))
# df4 <- cbind(time, df4)
# df4$time <- ymd_hms(df4$time )
# 
# obs_data <- left_join(obs_data, df4, by='time')
```

```{r data save}
#write.csv(obs_data, "C:\\Users\\Tarri\\Desktop\\portfolio_projects\\algal_bloom_dashboard\\data\\algae_data.csv")



```

# Summary of Initial Data

```{r read in data}
algae_data <- read_csv("C:\\Users\\Tarri\\Desktop\\portfolio_projects\\algal_bloom_dashboard\\data\\algae_data.csv")
summary(algae_data)
```


# Missing Data
From the missingness map, we can see that there is very little missing data, and it appears to only be from the ammonia column. Since a very small portion of data is missing, the rows with missing data were removed.

```{r missing data}
algae_data[algae_data == 0] <- NA

missmap(algae_data)
#algae_data <- algae_data[complete.cases(algae_data), ]
```

# Data Imputation
```{r data sub}

algae_sub <- algae_data %>%
  select(-c("...1","time","latitude","longitude"))
headers <- c("chlorophyll_flourescence_rfu","oxygen_saturation_fraction","bluegreen_algae_conc_ug.L",
             "bluegreen_algae_conc_rfu","chlorophyll_conc_kg.m3","oxygen_conc_kg.m3","temp_K","elec_cond_s.m",
             "pH","ammonia_ug.L","phosphate_ug.L", "phycocayanin_flour_rfu","nitrate_ug.L")
names(algae_sub) <- headers
algae_sub <- as.data.frame(sapply(algae_sub, abs))

algae_data1 <- amelia(algae_sub, m=5)
algae_sub <- algae_data1$imputations$imp1
algae_sub <- as.data.frame(sapply(algae_sub, abs))

summary(algae_sub)

```

# Correlation

```{r correlation}

rcorr(as.matrix(algae_sub, method="Spearman"))


```

# Pairs Plot

```{r pairs plot}
set.seed(42)

dims <- dplyr::select_if(algae_sub, is.numeric)
dims <- purrr::map2(dims, names(dims), ~list(values=.x, label=.y))
plot_ly(type = "splom", dimensions = setNames(dims, NULL), showupperhalf = FALSE, 
        diagonal = list(visible = FALSE) ,textangle = 45) %>% 
  layout( title='<b> Lake Erie water parameter Pairs-Plots </b>')

```

# Feature Selection
```{r boruta}
algae_bor <- algae_sub %>%
  select(-c('bluegreen_algae_conc_rfu'))

boruta <- Boruta(bluegreen_algae_conc_ug.L~.,data=algae_bor)

plot(boruta, xlab = "", xaxt = "n")
k <-lapply(1:ncol(boruta$ImpHistory),function(i)
  boruta$ImpHistory[is.finite(boruta$ImpHistory[,i]),i])
names(k) <- colnames(boruta$ImpHistory)
Labels <- sort(sapply(k,median))
axis(side = 1,las=2,labels = names(Labels),
       at = 1:ncol(boruta$ImpHistory), cex.axis = 0.7)


```


# Neural Network

```{r deep learning}
set.seed(55)
## take a subset of the original data which does not include the predicted variable 
## Also take a subset of just the predicted variable
dl_all_input <- algae_sub %>% select(-c('bluegreen_algae_conc_ug.L','bluegreen_algae_conc_rfu'))
dl_all_output <- algae_sub %>% select(c('bluegreen_algae_conc_ug.L'))

## Function to normalize all the columns in the dataset
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}

## Normalizes both the input and output dataset
dl_norm_input <-as.data.frame(lapply(dl_all_input, normalize))
dl_norm_output <-as.data.frame(lapply(dl_all_output, normalize))

## Randomly samples the indexes to create training and testing data
sub <- sample(nrow(dl_norm_output), floor(nrow(dl_norm_output)*0.90))

## Index the input and ouput datasets with the randomly generated indexes to get training and testing inputs/outputs
dl_train_input <- dl_norm_input[sub, ] 
dl_train_output <-dl_norm_output[sub, ]
dl_test_input <- dl_norm_input[-sub, ]
dl_test_output <-dl_norm_output[-sub, ]


## Turn all training and testing data into matrices
dl_train_mat <- as.matrix(dl_train_input)
dl_test_mat <- as.matrix(dl_test_input)
dl_train_output <- as.matrix(dl_train_output)
dl_test_output <-as.matrix(dl_test_output)

## Remove column names
colnames(dl_train_mat) <- NULL
colnames(dl_test_mat) <- NULL
```


```{r model build}
dl_model <- keras_model_sequential() 
act <- 'relu'
opt <- 'Adam'
loss <- 'mse'
met <- 'mse'
# Add layers to the model
set.seed(55)
dl_model %>% 
    layer_dense(units = 10, activation = act, input_shape = length(dl_train_input)) %>% 
    layer_dense(units = 120, activation = act) %>%
    layer_dropout(rate = 0.3) %>% 
    layer_dense(units = 1, activation = 'linear')

dl_model %>% compile(
     loss = loss,
     optimizer = opt,
     metrics = met
 )
summary(dl_model)

history <- dl_model %>% fit(
  dl_train_mat,
  dl_train_output,
  validation_split = 0.2,
  verbose = 0,
  epochs = 200
)

plot(history)


test_results <- dl_model %>% evaluate(
  dl_test_mat,
  dl_test_output,
  verbose = 0
)
test_results


test_predictions <- predict(dl_model, dl_test_mat)

df <- data.frame(prediction = as.numeric(test_predictions), concentration = dl_test_output)
colnames(df) <- c("prediction","concentration")
plot_ly() %>%
  add_markers(data=df, x=~prediction, y=~concentration, 
              name="Data Scatter", type="scatter", mode="markers") %>%
  add_trace(x = c(0,1), y = c(0,1), type="scatter", mode="lines",
        line = list(width = 4), name="Ideal Agreement") %>%
  layout(title=paste0('Scatterplot (Normalized) Observed vs. Predicted Values, Cor(Obs,Pred)=',
                      round(cor(df$prediction,df$concentration ), 2)),
           xaxis = list(title="NN (hidden=4) Predictions"),
           yaxis = list(title="(Normalized) Observed"),
           legend = list(orientation = 'h'))

cor(df$prediction, df$concentration)


```

```{r model save}
#save_model_hdf5(dl_model, "C:/Users/Tarri/Desktop/portfolio_projects/algal_bloom_dashboard/code/nn_model.hdf5")
```



```{r NN }
model = load_model_hdf5('C:/Users/Tarri/Desktop/portfolio_projects/algal_bloom_dashboard/code/nn_model.hdf5')
```

```{r }

test_results <- model %>% evaluate(
  dl_test_mat,
  dl_test_output,
  verbose = 0
)
test_results


test_predictions <- predict(model, dl_test_mat)

df <- data.frame(prediction = as.numeric(test_predictions), concentration = dl_test_output)

plot_ly() %>%
  add_markers(data=df, x=~prediction, y=~concentration, 
              name="Data Scatter", type="scatter", mode="markers") %>%
  add_trace(x = c(0,1), y = c(0,1), type="scatter", mode="lines",
        line = list(width = 4), name="Ideal Agreement") %>%
  layout(title=paste0('Scatterplot (Normalized) Observed vs. Predicted Values, Cor(Obs,Pred)=',
                      round(cor(df$prediction,df$concentration ), 2)),
           xaxis = list(title="NN (hidden=4) Predictions"),
           yaxis = list(title="(Normalized) Observed"),
           legend = list(orientation = 'h'))
```

```{r naive Bayes}
## Function to return concentration to its pre normalized value
unnormalize <- function(x) {
return((x*(max(dl_all_output$bluegreen_algae_conc_ug.L)-min(dl_all_output$bluegreen_algae_conc_ug.L))+min(dl_all_output$bluegreen_algae_conc_ug.L)))
}

## Create a data frame for new prediction values
pred <-as.data.frame(lapply(df, unnormalize))

pred$warning <- ifelse(pred$prediction > 1, "Danger", ifelse( (pred$prediction > 0.6 & pred$prediction <= 1), "Caution", "Safe"))


```

